# 项目日志

## 2021/4/20 

总结一下之前的研究进展。首先，使用edirect下载abstract数据然后直接清洗的这个思路基本上可以说是凉凉了。我们可以考虑迂回一下的思路，从已经筛选出来的abstract数据中提取出每个文章的pmid，然后通过爬虫去爬取数据，目前这个思路衍生出来有两个版本，一个是使用老师提供的PUBTATOR工具进行爬取，另一个是我自己手写的python爬虫脚本，感觉这两个都可以用，不同的在于，pubtator搞到的数据包含了许多高亮信息，但是需要做信息分离；我自己手写的爬虫虽然没有高亮信息，但是十分的纯净，基本不需要做分离。目前，我准备这两个代码都先执行着，由于数据量有35000多，所以大约需要执行3天左右。到时候可以再说具体怎么处理。

除开abstract之外，对于TO和gene的处理目前还处于一个比较初步的阶段，在这三天中，可以去处理一下这两部分的数据，提前写一下字符匹配的算法。个人希望是可以在五一节之前大体完成数据的计算，然后转入数据分析阶段，五一节完成数据分析部分。五一节后的期中假期去完成论文的施工。

## 2021/4/23

今天和孙梓淳进行了交流，孙梓淳使用的是R语言中的pubmed.mineR这个包，据他所说，这个包中的函数可以提取出文献中abstract中所有与gene相关的语句，这里我可以留意一下。

另外，项目二的同学安利了另一个包pubtatordb，这个包也要研究一下。目前，我使用的是python而这些小组使用的是R，我可以考虑结合或者吸收一下R在这方面的有点，同时结合一下我目前已经写好的python脚本。

除此之位，目前来说，在限制时间内完成36000个文献的检索，目前，准备考虑将文献数量削减到10000。也可以考虑使用并行化编程来解决这个问题，具体可以考虑和黄奇楠交流一下。

老师这里给了我一个奇怪的想法：可以考虑把gene-TO绘制成network然后对这个网络去进行分析，就使用系统合成生物学里面的知识就好。

参考工作：fun Rice Gene

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------这里是交流课后的尝试，首先pubmed.mineR这个包目前来看确实已经不能用，大概原因是pubmed改版后发生了改变，旧包使用的API已经无法链接到pubmed了

现在，我正在尝试第二个R包pubtatordb，这个包很有意思，它会在你的本地构建一个pubtator的数据库（本地化pubtator）然后之后的所有工作都是基于本地文件去操作的，如果接下来这条线也可以继续走下去的话，那么就从根本上解决了爬虫爬取数据过慢的问题了。11个G的本地数据库也不算很夸张，就是可能让黄奇楠去复现的时候会有一点麻烦，因为他也需要从头去构建这个数据库。

这里给出检索用的语句：
```python
pt_select(
  db_con,
  table_name,
  columns = NULL,
  keys = NULL,
  keytype = NULL,
  limit = Inf
)
```

————————————————————————————————————————————————————————————

包的数据损坏了，老老实实等pubtator吧

## 2021/4/26

pubtator的数据在昨天就全部下好了，今天对这个数据进行了清洗和整理。这里大致给一下整理的思路。

首先，整理的结果我是用Python中的字典进行保存的。字典的键是PMID，值是另一个字典包含了原文（合并了题目和摘要），分句信息（以list的形式记录每一句话开始和结尾的位置），注释信息（仅保留了gene相关的注释信息）。

其次，pubtator的数据使用|t|和|a|作为信息标签。所以，我们就可能读入四种不同类型的信息（空行，题目行，摘要行，注释行），明白了这一点就可以根据这个思路去写一个简单的数据清洗函数

 ```python
def pubtator_reader(dirpath):
    datapath = dirpath + 'data/'
    p_d = {}
    with open(datapath + 'result_MM_Pubtator.txt','r',encoding='utf-8') as f:
        for row in f:
            if '|t|' in row:
                key = row.split('|t|')[0]
                title = row.split('|t|')[1].strip()+' '
                p_d[key] = {}
                p_d[key]['paper'] = title
                p_d[key]['annotation'] = []
            elif '|a|' in row:
                key = row.split('|a|')[0]
                abstract = row.split('|a|')[1].strip()+' '
                p_d[key]['paper'] = p_d[key]['paper'] + abstract
                start = [0]
                stop = []
                for i in range(len(p_d[key]['paper'])):
                    if p_d[key]['paper'][i:i+2] == '. ':
                        stop.append(i+2)
                        start.append(i+2)
                del start[-1]
                sentence = []
                for i in range(len(start)):
                    sentence.append([start[i],stop[i]])
                p_d[key]['sentence'] = sentence
            elif row != '\n':
                note = row.strip().split('\t')
                if note[4] == 'Gene':
                    p_d[key]['annotation'].append(note)
    e = json.dumps(p_d)
    with open(datapath + 'pubtator_data.json','w') as f:
        f.write(e)
 ```

由于我们的项目只考虑基因和形状的共句关系，所以我们的注释信息仅保留gene的注释信息就好。在处理分句问题上，这里我还是纠结了一下下的，一开始想要直接把原文拆成句子保存，后来看了看注释信息，发现注释信息的loc信息是根据字符串所处的位置进行标注的，所以这里选择去记录句子的开始和结束位置，更加又有利于后续的句子筛选。

黄奇楠之前说的跨句表达可能就会有一点难做了，不是代码的复杂性而是内存很有可能会出现不够用的情况。

马上要去上课了，这里稍微确定一下后续的思路

1.  在已生成字典的基础上提取出所有包含基因的语句，将这些语句保存

2.  遍历这些语句去查看每个语句中是否还包含了TO的信息，将既含有TO也含有gene的语句提取出来

3.  在上一步操作的同时去记录每个gene-TO关系的频率（频次）频次A

4.  给这些基因和形状共句的句子构建依存树，去判断这两者究竟是单纯的出现在一个句子里，还是存在一定的联系。

5.  在上一步操作的同时去记录每个有明确关系的gene-TO关系的频率（频次）频次B
6.  对频次A和频次B进行一系列的统计学检验（具体做什么还没有想好）

7.  使用python中的networkx包绘制gene-TO互作网络，频次A对应无向图，频次B对应有向图

8.  最后，无论是使用python中的plt模块还是cytoscape来实现可视化

9. 撰写论文（最好是英文）

##  2021/5/10

可以不必追求结果的正确性，但是要尽量更多地使用上课老师所讲的技巧。目前的想法：词云+网络+依存树

   

   